{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07cf2f-b81d-43ef-907a-c49fed2d95ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from src.progressBar import printProgressBar\n",
    "\n",
    "import src.medicalDataLoader as medicalDataLoader\n",
    "import argparse\n",
    "from src.utils import *\n",
    "\n",
    "from src.UNet_Base import *\n",
    "import random\n",
    "import torch\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "\n",
    "# Select GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e976350-3fd5-4406-8511-86a06a9b4494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "220c7dcc-8438-454d-97b1-8e989a7b8f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def runTraining(epoch_num, weights_path='', augm = False):\n",
    "    print('-' * 40)\n",
    "    print('~~~~~~~~  Starting the training... ~~~~~~')\n",
    "    print('-' * 40)\n",
    "\n",
    "    ## DEFINE HYPERPARAMETERS (batch_size > 1)\n",
    "    batch_size = 16\n",
    "    batch_size_val = 16\n",
    "    lr = 0.001   # Learning Rate\n",
    "    epoch = epoch_num # Number of epochs\n",
    "    \n",
    "    root_dir = './Data/'\n",
    "\n",
    "    print(' Dataset: {} '.format(root_dir))\n",
    "\n",
    "    ## DEFINE THE TRANSFORMATIONS TO DO AND THE VARIABLES FOR TRAINING AND VALIDATION\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_set_full = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                      root_dir,\n",
    "                                                      transform=transform,\n",
    "                                                      mask_transform=mask_transform,\n",
    "                                                      augment=augm,  # Set to True to enable data augmentation\n",
    "                                                      equalize=False)\n",
    "\n",
    "    train_loader_full = DataLoader(train_set_full,\n",
    "                              batch_size=batch_size,\n",
    "                              worker_init_fn=np.random.seed(0),\n",
    "                              num_workers=0,\n",
    "                              shuffle=True)\n",
    "\n",
    "\n",
    "    val_set = medicalDataLoader.MedicalImageDataset('val',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    equalize=False)\n",
    "\n",
    "    val_loader = DataLoader(val_set,\n",
    "                            batch_size=batch_size_val,\n",
    "                            worker_init_fn=np.random.seed(0),\n",
    "                            num_workers=0,\n",
    "                            shuffle=False)\n",
    "\n",
    "\n",
    "    ## INITIALIZE YOUR MODEL\n",
    "    num_classes = 4 # NUMBER OF CLASSES\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\")\n",
    "    modelName = 'Test_Model'\n",
    "    print(\" Model Name: {}\".format(modelName))\n",
    "\n",
    "    ## CREATION OF YOUR MODEL\n",
    "    net = UNet(num_classes)\n",
    "    net = net.to(device)  # Move the model to the device\n",
    "\n",
    "    ## Load the weights from the previously trained model\n",
    "    if weights_path != '':\n",
    "        # previous_model_dir = './models/' + 'Test_Model' + '/' + str(epoch_num) +'_Epoch'\n",
    "        net.load_state_dict(torch.load(weights_path))\n",
    "        print(\" Model loaded: {}\".format(weights_path))\n",
    "\n",
    "    print(\"Total params: {0:,}\".format(sum(p.numel() for p in net.parameters() if p.requires_grad)))\n",
    "\n",
    "    # DEFINE YOUR OUTPUT COMPONENTS (e.g., SOFTMAX, LOSS FUNCTION, ETC)\n",
    "    softMax = torch.nn.Softmax(dim=1)\n",
    "    CE_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    ## PUT EVERYTHING IN GPU RESOURCES    \n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "        softMax.cuda()\n",
    "        CE_loss.cuda()\n",
    "\n",
    "    ## DEFINE YOUR OPTIMIZER\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    ### To save statistics ####\n",
    "    lossTotalTraining = []\n",
    "    Best_loss_val = 1000\n",
    "    BestEpoch = 0\n",
    "    \n",
    "    directory = 'Results/Statistics/' + modelName\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Starting the training ~~~~~~~~~~\")\n",
    "    if os.path.exists(directory)==False:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    ## START THE TRAINING\n",
    "    \n",
    "    ## FOR EACH EPOCH\n",
    "    for i in range(epoch):\n",
    "        net.train()\n",
    "        lossEpoch = []\n",
    "        DSCEpoch = []\n",
    "        DSCEpoch_w = []\n",
    "        num_batches = len(train_loader_full)\n",
    "        \n",
    "        ## FOR EACH BATCH\n",
    "        for j, data in enumerate(train_loader_full):\n",
    "            ### Set to zero all the gradients\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ## GET IMAGES, LABELS and IMG NAMES\n",
    "            images, labels, img_names = data            \n",
    "\n",
    "            ### From numpy to torch variables\n",
    "            labels = to_var(labels)\n",
    "            images = to_var(images)\n",
    "\n",
    "            ################### Train ###################\n",
    "            #-- The CNN makes its predictions (forward pass)\n",
    "            net_predictions = net(images)\n",
    "\n",
    "            #-- Compute the losses --#\n",
    "            # THIS FUNCTION IS TO CONVERT LABELS TO A FORMAT TO BE USED IN THIS CODE\n",
    "            segmentation_classes = getTargetSegmentation(labels)\n",
    "            \n",
    "            # COMPUTE THE LOSS\n",
    "            CE_loss_value = CE_loss(softMax(net_predictions), segmentation_classes) # XXXXXX and YYYYYYY are your inputs for the CE\n",
    "            lossTotal = CE_loss_value\n",
    "\n",
    "            # DO THE STEPS FOR BACKPROP (two things to be done in pytorch)\n",
    "            lossTotal.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # THIS IS JUST TO VISUALIZE THE TRAINING \n",
    "            lossEpoch.append(lossTotal.cpu().data.numpy())\n",
    "            printProgressBar(j + 1, num_batches,\n",
    "                             prefix=\"[Training] Epoch: {} \".format(i),\n",
    "                             length=15,\n",
    "                             suffix=\" Loss: {:.4f}, \".format(lossTotal))\n",
    "\n",
    "        lossEpoch = np.asarray(lossEpoch)\n",
    "        lossEpoch = lossEpoch.mean()\n",
    "\n",
    "        lossTotalTraining.append(lossEpoch)\n",
    "\n",
    "        printProgressBar(num_batches, num_batches,\n",
    "                             done=\"[Training] Epoch: {}, LossG: {:.4f}\".format(i,lossEpoch) + '\\n' \n",
    "                             + \"[Validation] Epoch: {}, LossG: {:.4f}\".format(i,lossEpoch))\n",
    "\n",
    "        \n",
    "        ## THIS IS HOW YOU WILL SAVE THE TRAINED MODELS AFTER EACH EPOCH. \n",
    "        ## WARNING!!!!! YOU DON'T WANT TO SAVE IT AT EACH EPOCH, BUT ONLY WHEN THE MODEL WORKS BEST ON THE VALIDATION SET!!\n",
    "        if not os.path.exists('./models/' + modelName):\n",
    "                os.makedirs('./models/' + modelName)\n",
    "\n",
    "        torch.save(net.state_dict(), './models/' + modelName + '/' + str(i) + '_Epoch')\n",
    "            \n",
    "        np.save(os.path.join(directory, 'Losses.npy'), lossTotalTraining)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runTraining(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a training loop\n",
    "# Looping from 9998 to 9994\n",
    "for i in range(9998, 9994, -1):\n",
    "    print(\"Epoch: {}\".format(i))\n",
    "    runTraining(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model on an Image\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotOutput(model_path, image_path):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ## CREATION OF YOUR MODEL\n",
    "    net_test = UNet(4)\n",
    "    net_test = net_test.to(device)\n",
    "    net_test.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    if image.mode != 'L':\n",
    "        image = image.convert('L')\n",
    "\n",
    "    # Preprocess the image\n",
    "    x = TF.to_tensor(image)\n",
    "    x = TF.normalize(x, [0.5], [0.5])\n",
    "    x = x.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Move the tensor to the same device as the model\n",
    "    x = x.to(device)\n",
    "\n",
    "    output = net_test(x)\n",
    "\n",
    "    # The output is the predicted segmentation\n",
    "    predicted_segmentation = torch.argmax(output.squeeze(), dim=0)\n",
    "\n",
    "    # Convert the tensor to a numpy array\n",
    "    predicted_segmentation = predicted_segmentation.detach().cpu().numpy()\n",
    "\n",
    "    # Display the predicted segmentation\n",
    "    plt.imshow(predicted_segmentation, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the training and validation loss of given weights\n",
    "\n",
    "def getLoss(weights_path):\n",
    "    # Load the weights from the file\n",
    "    model = UNet(4)\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "    # Determine the device and move the model to the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # DEFINE YOUR OUTPUT COMPONENTS (e.g., SOFTMAX, LOSS FUNCTION, ETC)\n",
    "    softMax = torch.nn.Softmax(dim=1)\n",
    "    CE_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the loss function\n",
    "    CE_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize a variable to store the total loss\n",
    "    root_dir = './Data/'\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_set_full = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                        root_dir,\n",
    "                                                        transform=transform,\n",
    "                                                        mask_transform=mask_transform,\n",
    "                                                        augment=True,  # Set to True to enable data augmentation\n",
    "                                                        equalize=False)\n",
    "\n",
    "    train_loader_full = DataLoader(train_set_full,\n",
    "                                batch_size=16,\n",
    "                                worker_init_fn=np.random.seed(0),\n",
    "                                num_workers=0,\n",
    "                                shuffle=True)\n",
    "\n",
    "\n",
    "    val_set = medicalDataLoader.MedicalImageDataset('val',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    equalize=False)\n",
    "\n",
    "    val_loader = DataLoader(val_set,                 \n",
    "                            batch_size=16,       \n",
    "                            worker_init_fn=np.random.seed(0),\n",
    "                            num_workers=0,\n",
    "                            shuffle=False)\n",
    "\n",
    "    lossEpoch_train = []\n",
    "    lossEpoch_val = []\n",
    "\n",
    "\n",
    "    for j, data in enumerate(train_loader_full):\n",
    "        ### Set to zero all the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        ## GET IMAGES, LABELS and IMG NAMES\n",
    "        images, labels, img_names = data            \n",
    "\n",
    "        ### From numpy to torch variables\n",
    "        labels = to_var(labels)\n",
    "        images = to_var(images)\n",
    "\n",
    "        ################### Train ###################\n",
    "        #-- The CNN makes its predictions (forward pass)\n",
    "        net_predictions = model(images)\n",
    "\n",
    "        #-- Compute the losses --#\n",
    "        # THIS FUNCTION IS TO CONVERT LABELS TO A FORMAT TO BE USED IN THIS CODE\n",
    "        segmentation_classes = getTargetSegmentation(labels)\n",
    "        \n",
    "        # COMPUTE THE LOSS\n",
    "        CE_loss_value = CE_loss(softMax(net_predictions), segmentation_classes) # XXXXXX and YYYYYYY are your inputs for the CE\n",
    "        \n",
    "        lossTotal = CE_loss_value\n",
    "        lossEpoch_train.append(lossTotal.item())\n",
    "\n",
    "    lossEpoch_train = np.asarray(lossEpoch_train)\n",
    "    lossEpoch_train = lossEpoch_train.mean()\n",
    "\n",
    "\n",
    "    for j, data in enumerate(val_loader):\n",
    "        ## GET IMAGES, LABELS and IMG NAMES\n",
    "        images, labels, img_names = data            \n",
    "\n",
    "        ### From numpy to torch variables\n",
    "        labels = to_var(labels)\n",
    "        images = to_var(images)\n",
    "\n",
    "        ################### Validate ###################\n",
    "        #-- The CNN makes its predictions (forward pass)\n",
    "        net_predictions = model(images)\n",
    "\n",
    "        #-- Compute the losses --#\n",
    "        # THIS FUNCTION IS TO CONVERT LABELS TO A FORMAT TO BE USED IN THIS CODE\n",
    "        segmentation_classes = getTargetSegmentation(labels)\n",
    "        \n",
    "        # COMPUTE THE LOSS\n",
    "        CE_loss_value = CE_loss(softMax(net_predictions), segmentation_classes) # XXXXXX and YYYYYYY are your inputs for the CE\n",
    "        \n",
    "        lossTotal = CE_loss_value\n",
    "        lossEpoch_val.append(lossTotal.item())\n",
    "\n",
    "    lossEpoch_val = np.asarray(lossEpoch_val)\n",
    "    lossEpoch_val = lossEpoch_val.mean()\n",
    "\n",
    "    print('Training loss:', lossEpoch_train)\n",
    "    print('Validation loss:', lossEpoch_val)\n",
    "    return lossEpoch_train, lossEpoch_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation loss for multiple models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# models_paths = ['./models/' + 'Trained' + '/Adam_Labeled_1.3k_Epoch']\n",
    "# models_paths = ['./models/' + 'Trained\\Data_Augmented' + '/Adam_Labeled_' + str(i) + '5k_Epoch' for i in range(0, 8)]\n",
    "models_paths = ['./models/' + 'Test_Model' + '/' + str(i) + '0_Epoch' for i in range(10,1500,100)]\n",
    "# List to store the losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Iterate over the list of paths\n",
    "for path in models_paths:\n",
    "    # Get the losses for the current path\n",
    "    train_loss, val_loss = getLoss(path)\n",
    "    \n",
    "    # Add the losses to the lists\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "# Plot the losses\n",
    "epochs = range(len(train_losses))\n",
    "# epochs = [(5 + 10*i)*1000 for i in range(len(train_losses))]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_losses, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the outputs for different models\n",
    "\n",
    "models_paths = ['./models/' + 'Test_Model' + '/' + str(i) + '999_Epoch' for i in range(1,15)]\n",
    "# models_paths = ['./models/' + 'Trained/Data_Augmented_15K_EpochPlus' + '/Adam_Labeled_' + str(i) + '5k_Epoch' for i in range(0, 8)]\n",
    "image_path = \"./Data/val/Img/patient061_01_1.png\"\n",
    "\n",
    "for model_path in models_paths:\n",
    "    plotOutput(model_path, image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised\n",
    "\n",
    "The objective is to implement a program based on this article [FixMatch](https://arxiv.org/abs/2001.07685). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Importing different Libraries to be able to treat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from src.progressBar import printProgressBar\n",
    "\n",
    "import src.medicalDataLoader as medicalDataLoader\n",
    "import argparse\n",
    "from src.utils import *\n",
    "\n",
    "from src.UNet_Base import *\n",
    "import random\n",
    "import torch\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which torch version is installed on the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "\n",
    "# Select GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We'll have one classic supervised model with no transformed image dataset. The results will be use as a a reference for the final model. The final model is built by comparing an none transformed image result to the result of the previous model and by comparing an output of this image and all transformed images based on this one and minimizing the difference.\n",
    "\n",
    "This way the model will not base his comparaison on the position in the image but will be able to identify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher\n",
    "\n",
    "The teacher is a basic supervised model where only labled data are beeing used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "def loadData(mode, batchSize, rootDir='./Data', _augment=False, _shuffle=False, _equalize=False,_numWorkers=0):\n",
    "    print(\"~~~~~~~~~~~ Loading Data ~~~~~~~~~~\")\n",
    "    \n",
    "    root_dir = './Data/'\n",
    "\n",
    "    print(' Dataset: {} '.format(root_dir))\n",
    "\n",
    "    ## DEFINE THE TRANSFORMATIONS TO DO AND THE VARIABLES FOR TRAINING AND VALIDATION\n",
    "    \n",
    "    _transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    _mask_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    _set = medicalDataLoader.MedicalImageDataset(mode,\n",
    "                                                      rootDir,\n",
    "                                                      transform=_transform,\n",
    "                                                      mask_transform=_mask_transform,\n",
    "                                                      augment=_augment,  # Set to True to enable data augmentation\n",
    "                                                      equalize=_equalize)\n",
    "\n",
    "    loader = DataLoader(_set,\n",
    "                              batch_size=batchSize,\n",
    "                              worker_init_fn=np.random.seed(0),\n",
    "                              num_workers=_numWorkers,\n",
    "                              shuffle=_shuffle)\n",
    "\n",
    "    return loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initModel(numberOfClasses, weights_path, modelName):\n",
    "    ## INITIALIZE YOUR MODEL\n",
    "    num_classes = 4 # NUMBER OF CLASSES\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\")\n",
    "    print(\" Model Name: {}\".format(modelName))\n",
    "\n",
    "    ## CREATION OF YOUR MODEL\n",
    "    net = UNet(num_classes)\n",
    "    net = net.to(device)  # Move the model to the device\n",
    "\n",
    "        ## Load the weights from the previously trained model\n",
    "    if weights_path != '':\n",
    "        net.load_state_dict(torch.load(weights_path))\n",
    "        print(\" Model loaded: {}\".format(weights_path))\n",
    "\n",
    "    print(\"Total params: {0:,}\".format(sum(p.numel() for p in net.parameters() if p.requires_grad)))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpuResources(net,softmax,CE_loss):\n",
    "    ## PUT EVERYTHING IN GPU RESOURCES    \n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "        softMax.cuda()\n",
    "        CE_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTraining(epoch_num, weights_path='', augm = False):\n",
    "    print('-' * 40)\n",
    "    print('~~~~~~~~  Starting the training... ~~~~~~')\n",
    "    print('-' * 40)\n",
    "\n",
    "    ## DEFINE HYPERPARAMETERS (batch_size > 1)\n",
    "    batch_size = 16\n",
    "    batch_size_val = 16\n",
    "    lr = 0.001   # Learning Rate\n",
    "    epoch = epoch_num # Number of epochs\n",
    "    \n",
    "    train_loader_full = loadData('train', batch_size,_augment=augm, _shuffle=True)\n",
    "    val_loader = loadData('val', batch_size_val,_augment=augm)\n",
    "\n",
    "    modelName = 'Test_Model'\n",
    "    net = initModel(4, weights_path,modelName)\n",
    "\n",
    "    # DEFINE YOUR OUTPUT COMPONENTS (e.g., SOFTMAX, LOSS FUNCTION, ETC)\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    CE_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    gpuResources(net,softmax,CE_loss)\n",
    "\n",
    "    ## DEFINE YOUR OPTIMIZER\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    ### To save statistics ####\n",
    "    lossTotalTraining = []\n",
    "    Best_loss_val = 1000\n",
    "    BestEpoch = 0\n",
    "    \n",
    "\n",
    "    print(\"~~~~~~~~~~~ Starting the training ~~~~~~~~~~\")\n",
    "    # Create saving directory if not already existing\n",
    "    directory = 'Results/Statistics/' + modelName\n",
    "    if os.path.exists(directory)==False:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    ## START THE TRAINING\n",
    "    \n",
    "    ## FOR EACH EPOCH\n",
    "    for i in range(epoch):\n",
    "        net.train()\n",
    "        lossEpoch = []\n",
    "        DSCEpoch = []\n",
    "        DSCEpoch_w = []\n",
    "        num_batches = len(train_loader_full)\n",
    "        \n",
    "        ## FOR EACH BATCH\n",
    "        for j, data in enumerate(train_loader_full):\n",
    "            ### Set to zero all the gradients\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ## GET IMAGES, LABELS and IMG NAMES\n",
    "            images, labels, img_names = data            \n",
    "\n",
    "            ### From numpy to torch variables\n",
    "            labels = to_var(labels)\n",
    "            images = to_var(images)\n",
    "\n",
    "            ################### Train ###################\n",
    "            #-- The CNN makes its predictions (forward pass)\n",
    "            net_predictions = net(images)\n",
    "\n",
    "            #-- Compute the losses --#\n",
    "            # THIS FUNCTION IS TO CONVERT LABELS TO A FORMAT TO BE USED IN THIS CODE\n",
    "            segmentation_classes = getTargetSegmentation(labels)\n",
    "            \n",
    "            # COMPUTE THE LOSS\n",
    "            CE_loss_value = CE_loss(softmax(net_predictions), segmentation_classes) # XXXXXX and YYYYYYY are your inputs for the CE\n",
    "            lossTotal = CE_loss_value\n",
    "\n",
    "            # DO THE STEPS FOR BACKPROP (two things to be done in pytorch)\n",
    "            lossTotal.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # THIS IS JUST TO VISUALIZE THE TRAINING \n",
    "            lossEpoch.append(lossTotal.cpu().data.numpy())\n",
    "            printProgressBar(j + 1, num_batches,\n",
    "                             prefix=\"[Training] Epoch: {} \".format(i),\n",
    "                             length=15,\n",
    "                             suffix=\" Loss: {:.4f}, \".format(lossTotal))\n",
    "\n",
    "        lossEpoch = np.asarray(lossEpoch)\n",
    "        lossEpoch = lossEpoch.mean()\n",
    "\n",
    "        lossTotalTraining.append(lossEpoch)\n",
    "\n",
    "        printProgressBar(num_batches, num_batches,\n",
    "                             done=\"[Training] Epoch: {}, LossG: {:.4f}\".format(i,lossEpoch) + '\\n' \n",
    "                             + \"[Validation] Epoch: {}, LossG: {:.4f}\".format(i,lossEpoch))\n",
    "\n",
    "        \n",
    "        ## THIS IS HOW YOU WILL SAVE THE TRAINED MODELS AFTER EACH EPOCH. \n",
    "        ## WARNING!!!!! YOU DON'T WANT TO SAVE IT AT EACH EPOCH, BUT ONLY WHEN THE MODEL WORKS BEST ON THE VALIDATION SET!!\n",
    "        if not os.path.exists('./models/' + modelName):\n",
    "                os.makedirs('./models/' + modelName)\n",
    "\n",
    "        torch.save(net.state_dict(), './models/' + modelName + '/' + str(i) + '_Epoch')\n",
    "            \n",
    "        np.save(os.path.join(directory, 'Losses.npy'), lossTotalTraining)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "~~~~~~~~  Starting the training... ~~~~~~\n",
      "----------------------------------------\n",
      "~~~~~~~~~~~ Loading Data ~~~~~~~~~~\n",
      " Dataset: ./Data/ \n",
      "~~~~~~~~~~~ Loading Data ~~~~~~~~~~\n",
      " Dataset: ./Data/ \n",
      "~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\n",
      " Model Name: Test_Model\n",
      "Total params: 60,664\n",
      "~~~~~~~~~~~ Starting the training ~~~~~~~~~~\n",
      "[Training] Epoch: 0 [DONE]                                 \n",
      "[Training] Epoch: 0, LossG: 1.3650===================================================================] 100.0%\n",
      "[Validation] Epoch: 0, LossG: 1.3650                                      \n",
      "[Training] Epoch: 1 [DONE]                                 \n",
      "[Training] Epoch: 1, LossG: 1.3371===================================================================] 100.0%\n",
      "[Validation] Epoch: 1, LossG: 1.3371                                      \n",
      "[Training] Epoch: 2 [DONE]                                 \n",
      "[Training] Epoch: 2, LossG: 1.3144===================================================================] 100.0%\n",
      "[Validation] Epoch: 2, LossG: 1.3144                                      \n",
      "[Training] Epoch: 3 [DONE]                                 \n",
      "[Training] Epoch: 3, LossG: 1.2927===================================================================] 100.0%\n",
      "[Validation] Epoch: 3, LossG: 1.2927                                      \n",
      "[Training] Epoch: 4 [DONE]                                 \n",
      "[Training] Epoch: 4, LossG: 1.2712===================================================================] 100.0%\n",
      "[Validation] Epoch: 4, LossG: 1.2712                                      \n",
      "[Training] Epoch: 5 [DONE]                                 \n",
      "[Training] Epoch: 5, LossG: 1.2488===================================================================] 100.0%\n",
      "[Validation] Epoch: 5, LossG: 1.2488                                      \n",
      "[Training] Epoch: 6 [DONE]                                 \n",
      "[Training] Epoch: 6, LossG: 1.2255===================================================================] 100.0%\n",
      "[Validation] Epoch: 6, LossG: 1.2255                                      \n",
      "[Training] Epoch: 7 [DONE]                                 \n",
      "[Training] Epoch: 7, LossG: 1.2019===================================================================] 100.0%\n",
      "[Validation] Epoch: 7, LossG: 1.2019                                      \n",
      "[Training] Epoch: 8 [DONE]                                 \n",
      "[Training] Epoch: 8, LossG: 1.1785===================================================================] 100.0%\n",
      "[Validation] Epoch: 8, LossG: 1.1785                                      \n",
      "[Training] Epoch: 9 [DONE]                                 \n",
      "[Training] Epoch: 9, LossG: 1.1558===================================================================] 100.0%\n",
      "[Validation] Epoch: 9, LossG: 1.1558                                      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Results/Statistics/Test_Model/Losses.npy'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runTraining(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the training and validation loss of given weights\n",
    "\n",
    "def getLoss(weights_path):\n",
    "    # Load the weights from the file\n",
    "    model = UNet(4)\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "    # Determine the device and move the model to the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # DEFINE YOUR OUTPUT COMPONENTS (e.g., SOFTMAX, LOSS FUNCTION, ETC)\n",
    "    softMax = torch.nn.Softmax(dim=1)\n",
    "    CE_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the loss function\n",
    "    CE_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader_full = loadData('train',batchSize=16,_augment=True,_shuffle=True)\n",
    "    val_loader = loadData('train',batchSize=16)\n",
    "\n",
    "\n",
    "    lossEpoch_train = []\n",
    "    lossEpoch_val = []\n",
    "\n",
    "\n",
    "    for j, data in enumerate(train_loader_full):\n",
    "        ### Set to zero all the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        ## GET IMAGES, LABELS and IMG NAMES\n",
    "        images, labels, img_names = data            \n",
    "\n",
    "        ### From numpy to torch variables\n",
    "        labels = to_var(labels)\n",
    "        images = to_var(images)\n",
    "\n",
    "        ################### Train ###################\n",
    "        #-- The CNN makes its predictions (forward pass)\n",
    "        net_predictions = model(images)\n",
    "\n",
    "        #-- Compute the losses --#\n",
    "        # THIS FUNCTION IS TO CONVERT LABELS TO A FORMAT TO BE USED IN THIS CODE\n",
    "        segmentation_classes = getTargetSegmentation(labels)\n",
    "        \n",
    "        # COMPUTE THE LOSS\n",
    "        CE_loss_value = CE_loss(softMax(net_predictions), segmentation_classes) # XXXXXX and YYYYYYY are your inputs for the CE\n",
    "        \n",
    "        lossTotal = CE_loss_value\n",
    "        lossEpoch_train.append(lossTotal.item())\n",
    "\n",
    "    lossEpoch_train = np.asarray(lossEpoch_train)\n",
    "    lossEpoch_train = lossEpoch_train.mean()\n",
    "\n",
    "\n",
    "    for j, data in enumerate(val_loader):\n",
    "        ## GET IMAGES, LABELS and IMG NAMES\n",
    "        images, labels, img_names = data            \n",
    "\n",
    "        ### From numpy to torch variables\n",
    "        labels = to_var(labels)\n",
    "        images = to_var(images)\n",
    "\n",
    "        ################### Validate ###################\n",
    "        #-- The CNN makes its predictions (forward pass)\n",
    "        net_predictions = model(images)\n",
    "\n",
    "        #-- Compute the losses --#\n",
    "        # THIS FUNCTION IS TO CONVERT LABELS TO A FORMAT TO BE USED IN THIS CODE\n",
    "        segmentation_classes = getTargetSegmentation(labels)\n",
    "        \n",
    "        # COMPUTE THE LOSS\n",
    "        CE_loss_value = CE_loss(softMax(net_predictions), segmentation_classes) # XXXXXX and YYYYYYY are your inputs for the CE\n",
    "        \n",
    "        lossTotal = CE_loss_value\n",
    "        lossEpoch_val.append(lossTotal.item())\n",
    "\n",
    "    lossEpoch_val = np.asarray(lossEpoch_val)\n",
    "    lossEpoch_val = lossEpoch_val.mean()\n",
    "\n",
    "    print('Training loss:', lossEpoch_train)\n",
    "    print('Validation loss:', lossEpoch_val)\n",
    "    return lossEpoch_train, lossEpoch_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model on an Image\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotOutput(model_path, image_path):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ## CREATION OF YOUR MODEL\n",
    "    net_test = UNet(4)\n",
    "    net_test = net_test.to(device)\n",
    "    net_test.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    if image.mode != 'L':\n",
    "        image = image.convert('L')\n",
    "\n",
    "    # Preprocess the image\n",
    "    x = TF.to_tensor(image)\n",
    "    x = TF.normalize(x, [0.5], [0.5])\n",
    "    x = x.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Move the tensor to the same device as the model\n",
    "    x = x.to(device)\n",
    "\n",
    "    output = net_test(x)\n",
    "\n",
    "    # The output is the predicted segmentation\n",
    "    predicted_segmentation = torch.argmax(output.squeeze(), dim=0)\n",
    "\n",
    "    # Convert the tensor to a numpy array\n",
    "    predicted_segmentation = predicted_segmentation.detach().cpu().numpy()\n",
    "\n",
    "    # Display the predicted segmentation\n",
    "    plt.imshow(predicted_segmentation, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/Test_Model/2999_Epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/paulsade/Development/UNet_Semi_Supervised_Medical_Image_Segmentation/mainSegmentationChallenge.ipynb Cellule 28\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paulsade/Development/UNet_Semi_Supervised_Medical_Image_Segmentation/mainSegmentationChallenge.ipynb#Y121sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m image_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./Data/val/Img/patient061_01_1.png\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paulsade/Development/UNet_Semi_Supervised_Medical_Image_Segmentation/mainSegmentationChallenge.ipynb#Y121sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m model_path \u001b[39min\u001b[39;00m models_paths:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/paulsade/Development/UNet_Semi_Supervised_Medical_Image_Segmentation/mainSegmentationChallenge.ipynb#Y121sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     plotOutput(model_path, image_path)\n",
      "\u001b[1;32m/Users/paulsade/Development/UNet_Semi_Supervised_Medical_Image_Segmentation/mainSegmentationChallenge.ipynb Cellule 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paulsade/Development/UNet_Semi_Supervised_Medical_Image_Segmentation/mainSegmentationChallenge.ipynb#Y121sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m net_test \u001b[39m=\u001b[39m UNet(\u001b[39m4\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paulsade/Development/UNet_Semi_Supervised_Medical_Image_Segmentation/mainSegmentationChallenge.ipynb#Y121sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m net_test \u001b[39m=\u001b[39m net_test\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/paulsade/Development/UNet_Semi_Supervised_Medical_Image_Segmentation/mainSegmentationChallenge.ipynb#Y121sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m net_test\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(model_path))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paulsade/Development/UNet_Semi_Supervised_Medical_Image_Segmentation/mainSegmentationChallenge.ipynb#Y121sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Load the image\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paulsade/Development/UNet_Semi_Supervised_Medical_Image_Segmentation/mainSegmentationChallenge.ipynb#Y121sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(image_path)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    436\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/Test_Model/2999_Epoch'"
     ]
    }
   ],
   "source": [
    "# Displaying the outputs for different models\n",
    "\n",
    "models_paths = ['./models/' + 'Test_Model' + '/' + str(i) + '999_Epoch' for i in range(1,15)]\n",
    "# models_paths = ['./models/' + 'Trained/Data_Augmented_15K_EpochPlus' + '/Adam_Labeled_' + str(i) + '5k_Epoch' for i in range(0, 8)]\n",
    "image_path = \"./Data/val/Img/patient061_01_1.png\"\n",
    "\n",
    "for model_path in models_paths:\n",
    "    plotOutput(model_path, image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
